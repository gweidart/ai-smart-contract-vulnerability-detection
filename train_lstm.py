# train_lstm.py

import torch
import torch.nn as nn
from torch.optim import Adam
from rich_utils import add_debug_message
from model_utils import save_model

def train_and_evaluate_lstm(model, train_loader, test_loader, num_epochs=10):
    criterion = nn.CrossEntropyLoss()
    optimizer = Adam(model.parameters(), lr=0.001)

    try:
        for epoch in range(num_epochs):
            model.train()
            for data, labels in train_loader:
                try:
                    data, labels = data.to(model.device), labels.to(model.device)
                    optimizer.zero_grad()
                    outputs = model(data)
                    loss = criterion(outputs, labels)
                    loss.backward()
                    optimizer.step()
                except Exception as e:
                    add_debug_message(f"[bold red]Error during training at epoch {epoch + 1}: {e}")
            
            add_debug_message(f"Epoch {epoch + 1}/{num_epochs} - Loss: {loss.item()}")

            model.eval()
            correct = 0
            total = 0
            try:
                with torch.no_grad():
                    for data, labels in test_loader:
                        data, labels = data.to(model.device), labels.to(model.device)
                        outputs = model(data)
                        _, predicted = torch.max(outputs.data, 1)
                        total += labels.size(0)
                        correct += (predicted == labels).sum().item()
                
                accuracy = 100 * correct / total
                add_debug_message(f"Accuracy: {accuracy}%")
            except Exception as e:
                add_debug_message(f"[bold red]Error during evaluation at epoch {epoch + 1}: {e}")

        save_model(model, model_name='lstm_model')
    except Exception as e:
        add_debug_message(f"[bold red]Error in train_and_evaluate_lstm: {e}")
    
    return model
