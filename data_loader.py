import os
from concurrent.futures import ThreadPoolExecutor
import joblib
from labeler import label_vulnerabilities
from rich_utils import add_debug_message, create_progress_bar

def load_file(file_path):
    try:
        with open(file_path, 'r') as file:
            source_code = file.read()
            labels = label_vulnerabilities(source_code)
            return source_code, labels
    except Exception as e:
        add_debug_message(f"[bold red]Error loading file {file_path}: {e}")
        return None, None

def load_dataset(dataset_dir, batch_size=250, cache_file='dataset_cache.joblib'):
    data = []
    labels = []
    all_data = []
    all_labels = []
    try:
        add_debug_message("[bold yellow]Loading dataset...")
        with ThreadPoolExecutor() as executor:
            futures = []
            for i in range(1, 44):  # Loop through contract1 to contract42
                folder_name = f"contract{i}"
                folder_path = os.path.join(dataset_dir, folder_name)
                if os.path.isdir(folder_path):
                    for file_name in os.listdir(folder_path):
                        if file_name.endswith('.sol'):
                            file_path = os.path.join(folder_path, file_name)
                            futures.append(executor.submit(load_file, file_path))
            
            with create_progress_bar() as progress:
                task = progress.add_task("Loading files...", total=len(futures))
                for future in futures:
                    source_code, file_labels = future.result()
                    if source_code is not None and file_labels is not None:
                        data.append(source_code)
                        labels.append(file_labels)
                    progress.advance(task)

                    # Process data in batches
                    if len(data) >= batch_size:
                        all_data.extend(data)
                        all_labels.extend(labels)
                        data = []
                        labels = []

        # Save the final batch
        if data:
            all_data.extend(data)
            all_labels.extend(labels)
        
        # Cache the entire dataset
        joblib.dump((all_data, all_labels), cache_file)
        add_debug_message(f"[bold green]Dataset loaded successfully. Number of data samples: {len(all_data)}, Number of label samples: {len(all_labels)}")
    except Exception as e:
        add_debug_message(f"[bold red]Error loading dataset: {e}")
    
    # Ensure labels are not flattened
    return all_data, all_labels
